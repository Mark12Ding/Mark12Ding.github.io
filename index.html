<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Shuangrui Ding</title>
<meta name="author" content="Shuangrui Ding">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" type="text/css" href="stylesheet.css">
<link rel="icon" type="image/png" href="images/icon.png">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shuangrui Ding (丁双睿)</name>
              </p>
              <p>I am an incoming Ph.D. student in <a href="http://mmlab.ie.cuhk.edu.hk/">Multi-Media Lab</a> at The Chinese University of Hong Kong, supervised by <a href="http://dahua.site/">Prof. Dahua Lin</a>. My current research insterst spans on the video understanding and self-supervised learning.
              </p>
              <p>I obtained my Master's degree in Electrical Engineering from <a href="https://ee.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a> in 2023, where I was advised by <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14" target="_blank">Prof. Hongkai Xiong</a>. Prior to that, I earned a Bachelor's degree in Computer Science from the <a href="https://majors.engin.umich.edu/program/computer-science/" target="_blank">University of Michigan</a>, along with a dual degree in Electrical and Computer Engineering from <a href="https://www.ji.sjtu.edu.cn/academics/undergraduate-program/degrees-programs/electrical-and-computer-engineering/" target="_blank">Shanghai Jiao Tong University</a> in 2021.
              </p>
              <p>
                Please drop me an email if you are interested in corporation with we.  
              </p>
              <p style="text-align:center">
                <a href="mailto:dsr1212@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShuangruiDing.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=jmFZZYMAAAAJ">Google Scholar</a>&nbsp/&nbsp
                <a href="https://github.com/Mark12Ding">Github</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/selfie.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/selfie_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <strong>[Jul. 2023]</strong> </b> Two papers have been accepted at <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>. One focuses on video Transformer pruning, and the other on self-supervised video object discovery.
              </p>
              <p>
                <strong>[Jun. 2023]</strong> </b> Graduate from SJTU in just two years by exemption! Immensely grateful to my advisor, Prof. Xiong, for providing invaluable support throughout my master's journey.</a>
              </p>
              <!-- <p>
                <strong>[Jul. 2022]</strong> </b> Our paper <em>Static and Dynamic Concepts for Self-supervised Video Representation Learning</em> was accepted at <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </p>
              <p>
                <strong>[Jun. 2022]</strong> </b> Our paper <em>Dual Contrastive Learning for Spatio-temporal Representation</em> was accepted at <a href="https://2022.acmmm.org/">ACM MM 2022</a>.
              </p>
              <p>
                <strong>[Mar. 2022]</strong> </b> Our paper <em>Motion-aware Contrastive Video Representation Learning via Foreground-background Merging
                    </em> was accepted at <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Motion-Aware_Contrastive_Video_Representation_Learning_via_Foreground-Background_Merging_CVPR_2022_paper.html">CVPR 2022</a>.
              </p> -->
              <!-- <p>
                <strong>[Jul. 2021]</strong> </b> Our paper <em>Enhancing Self-Supervised Video Representation Learning via Multi-Level Feature Optimization
                    </em> was accepted at <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Qian_Enhancing_Self-Supervised_Video_Representation_Learning_via_Multi-Level_Feature_Optimization_ICCV_2021_paper.html">ICCV 2021</a>.
              </p> -->
              <!-- <p>
                <strong>[Sep. 2020]</strong> </b> Our paper <em>Towards More
                     Practical Adversarial Attacks on Graph Neurel Networks
                    </em> was accepted at <a href="https://papers.nips.cc/paper/2020/hash/32bb90e8976aab5298d5da10fe66f21d-Abstract.html">NeurIPS 2020</a>.
              </p> -->
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:baseline">
          <heading>Preprint</heading>
        </td>
      </tr>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/arxiv_arch.png' width="160">
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Motion-inductive Self-supervised Object Discovery in Videos</papertitle>
                <br>
                <strong>Shuangrui Ding</strong>,
                <a href="https://weidixie.github.io/">Weidi Xie</a>,
                Yabo Chen,
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ud6aBAcAAAAJ">Xiaopeng Zhang</a>,
                <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>,
                <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ">Qi Tian</a>
                <br>
                  <em>preprint</em>, 2022 
                <br>
                <a href="https://arxiv.org/abs/2210.00221">arXiv</a> 
                <p></p>
                <p> We propose a motion-inductive model through directly processing consecutive RGB frames to segment the foreground objects and train it without any mask annotations.</p>
              </td>
            </tr> 
        </tbody></table>




    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:baseline">
          <heading>Publications(&#42; equal contribution)</heading>
        </td>
      </tr>
    </tbody></table>
        



    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ICCV23_token.png' width="160" height="80">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation</papertitle>
                <br>
                <strong>Shuangrui Ding</strong>,
                <a href="https://scholar.google.com/citations?user=hCr8Km8AAAAJ&hl=zh-CN">Peisen Zhao</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ud6aBAcAAAAJ">Xiaopeng Zhang</a>,
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>,
                <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ">Qi Tian</a>
                <br>
        <em>ICCV</em>, 2023
                <br>
                More to come. Stay tuned.
                <p></p>
                <p> Propose token pruning strategy for video Transformers to offer a competitive speed-accuracy trade-off without additional training or parameters.</p>
              </td>
        </tr>
        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ICCV23_ssl.png' width="160" height="100">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos</papertitle>
                <br>
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <strong>Shuangrui Ding</strong>, 
                Xian Liu,
                <a href="http://dahua.site/">Dahua Lin</a>
                <br>
        <em>ICCV</em>, 2023
                <br>
                More to come. Stay tuned.
                <p></p>
                <p> Jointly utilizes high-level semantics and low-level temporal correspondence for object-centric learning in videos without any supervision. </p>
              </td>
        </tr>
        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ECCV22.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Static and Dynamic Concepts for Self-supervised Video Representation Learning</papertitle>
                <br>
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <strong>Shuangrui Ding</strong>, Xian Liu,
                <a href="http://dahua.site/">Dahua Lin</a>
                <br>
        <em>ECCV</em>, 2022 
                <br>
                <a href="https://arxiv.org/abs/2207.12795">arXiv</a> 
                /
                <a href="https://github.com/shvdiwnkozbw/Self-supervised-Video-Concept">code</a>
                /
                <a href="data/eccv22.pdf">slide</a>
                <p></p>
                <p> Learn static and dynamic visual concepts in videos to aggregate local patterns with similar semantics to boost unsupervised video representation.</p>
              </td>
        </tr> 
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MM2022.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Dual Contrastive Learning for Spatio-temporal Representation</papertitle>
              <br>
              <strong>Shuangrui Ding</strong>,
              <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
              <br>
        <em>ACM MM</em>, 2022 
              <br>
              <a href="https://arxiv.org/abs/2207.05340">arXiv</a> 
              /
              <a href="data/acmmm2022_poster_mmfp0200.pdf">poster</a>
              /
              <a href="data/MM22-mmfp0200.mp4">video</a>
              /
              <a href="https://github.com/Mark12Ding/DCLR">code</a>
              
              <p></p>
              <p> Present a novel dual contrastive formulation to decouple the static/dynamic features and thus mitigate the background bias.</p>
            </td>
      </tr> 
      <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CVPR2022.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Motion-aware Contrastive Video Representation Learning via Foreground-background Merging</papertitle>
              <br>
              <strong>Shuangrui Ding</strong>,
              Maomao Li, Tianyu Yang, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, Haohang Xu, Qingyi Chen, Jue Wang, <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
              <br>
        <em>CVPR</em>, 2022 
              <br>
              <a href="https://mark12ding.github.io/project/CVPR22_FAME/">Project page</a>
              / 
              <a href="https://arxiv.org/abs/2109.15130/">arXiv</a> 
              / 
              <a href="https://github.com/Mark12Ding/FAME">code</a>
              /
              <a href="https://mp.weixin.qq.com/s/eZ_8qyVa7L8-n2RHBge0mQ">Chinese coverage</a>
              /
              <a href="data/CVPR_4560_poster.pdf">poster</a>
              <p></p>
              <p>Mitigate the background bias in self-supervised video representation learning via copy-pasting the foreground onto the other backgrounds.</p>
            </td>
      </tr>
      <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='iccv_image'>
                  <img src='images/iccv2.jpeg' width="160"></div>
                <img src='images/iccv.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function iccv_start() {
                  document.getElementById('iccv_image').style.opacity = "1";
                }

                function iccv_stop() {
                  document.getElementById('iccv_image').style.opacity = "0";
                }
                iccv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</papertitle>
              <br>
              <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, Yuxi Li, Huabin Liu, John See,
              <strong>Shuangrui Ding</strong>, Xian Liu, Dian Li, 
              <a href="https://weiyaolin.github.io/">Weiyao Lin</a>
              <br>
        <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.02183">arXiv</a> / 
              <a href="https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization">code</a>
              <p></p>
              <p>Self-supervised video representation learning from the perspective of both high-level semantics and lower-level characteristics</p>
            </td>
      </tr>
      <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/NIPS2020.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Towards More Practical Adversarial Attacks on Graph Neural Networks</papertitle>
              <br>
              <a href="http://www.jiaqima.com/">Jiaqi Ma*</a>,
              <strong>Shuangrui Ding*</strong>,
              <a href="http://www-personal.umich.edu/~qmei">Qiaozhu Mei</a>
              <br>
        <em>NeurIPS</em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/2006.05057">arXiv</a>
        /
              <a href="https://docs.google.com/presentation/d/18AqRjgTz8d2sSCE_cUgTHYzs2mWhOug-U5HZWBUU67o/edit?usp=sharing">slides</a>
        /    
              <a href="https://slideslive.com/38931435/practical-adversarial-attacks-on-graph-neural-networks">video
              </a>
        /
              <a href="https://github.com/Mark12Ding/GNN-Practical-Attack">code
              </a>
              <p></p>
              <p>Exploiting the structural inductive biases of GNNs, the restricted black-box adversarial attacks can be conducted effectively.</p>
            </td>
      </tr> 
      </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards</heading>
              <p>
                <strong>CUHK Vice-Chancellor's Ph.D. Scholarship</strong> (80,000 HKD), Graduate school of CUHK. 2023 
                <!-- [<a href="https://mp.weixin.qq.com/s/8hZz15kjuEoPP_aCZ2sSRQ">coverage</a>] -->
              </p>    
              
              <p>
                <strong>Graduate National Scholarship</strong> (Top 2%), Ministry of Education of China. 2022 
                <!-- [<a href="https://mp.weixin.qq.com/s/8hZz15kjuEoPP_aCZ2sSRQ">coverage</a>] -->
              </p>              
              <p>
                <strong>Shanghai Excellent Graduate</strong> (Top 5%), Shanghai Municipal Education Commission. 2021 
              </p>
              <p>
                <strong>Finalist winner</strong> (Top 0.3%), Mathematical Contest in Modeling. 2019 
              </p>
              <p>
              <strong>National Scholarship</strong> (Top 2%), Ministry of Education of China. 2018 
              </p>
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Services</heading>
              <p>
                  <li>
                      Reviewer: ECCV'22, AAAI'23, ICLR'23, CVPR'23, ICCV'23, ACM MM'23, NeruIPS'23.
                  </li>
              </p>
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Misc</heading>
              <p>
                1. My favorite sports is soccer. I was the captain of UM-SJTU JI soccer team during season 2018. Besides, I am a super fan of Manchester City in Premier League. Kudos to The Treble&#127942; &#127942; &#127942;
              </p>
              <p>
                2. I am proud that I have graudated from the competition class at Hangzhou No.2 High school, where I make friends with so many talented students and prestigious teachers. 
              </p>
              <p>
                3. It is worth mentioning that <a href="https://shvdiwnkozbw.github.io/">Rui</a> is my best friend and has motivated me forward for over ten years as my role model. Best wishes and good luck!
              </p>
            </td>
          </tr>
        </tbody></table>
<!--     <table width="50%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td style="padding:0px">
                    <br>
                    <br>
                    <div>
                        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=7lqf9KytMX7LblPx6VWALqJ3PbQAiEEYCwUq_KvZgBI'></script>
                    </div>
                </td>
            </tr>
        </tbody>
    </table> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td style="padding:0px">
                    <p font-size:small;>
                        <br>
                        <br>
                        <div style="float:left;">
                            Updated at Jul. 2023
                        </div>
                        <div style="float:right;">
                            Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                        </div>
                        <br>
                        <br>        
                    </p>                           
                </td>
            </tr>
        </tbody>
    </table>
    </body>
</html>
