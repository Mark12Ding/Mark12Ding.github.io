<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Shuangrui Ding</title>
<meta name="author" content="Shuangrui Ding">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" type="text/css" href="stylesheet.css">
<link rel="icon" type="image/png" href="images/icon.png">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shuangrui Ding (丁双睿)</name>
              </p>
              <p>I am a first-year Ph.D. student in the <a href="http://mmlab.ie.cuhk.edu.hk/">Multi-Media Lab</a> at The Chinese University of Hong Kong, supervised by <a href="http://dahua.site/">Prof. Dahua Lin</a>. My current research interest spans the large language model and song generation.
              </p>
              <p>I obtained my Master's degree in Electrical Engineering from <a href="https://ee.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a> in 2023, where I was advised by <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14" target="_blank">Prof. Hongkai Xiong</a>. Prior to that, I earned a Bachelor's degree in Computer Science from the <a href="https://majors.engin.umich.edu/program/computer-science/" target="_blank">University of Michigan</a>, along with a dual degree in Electrical and Computer Engineering from <a href="https://www.ji.sjtu.edu.cn/academics/undergraduate-program/degrees-programs/electrical-and-computer-engineering/" target="_blank">Shanghai Jiao Tong University</a> in 2021.
              </p>
              <p style="text-align:center">
                <a href="mailto:mark12ding@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShuangruiDing.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=jmFZZYMAAAAJ">Google Scholar</a>&nbsp/&nbsp
                <a href="https://github.com/Mark12Ding">Github</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/selfie.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/selfie_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <strong>[Jul. 2024]</strong> </b> Three paper have been accepted at <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a>.
              </p>
              <p>
                <strong>[May. 2024]</strong> </b> One paper have been accepted at <a href="https://icml.cc/Conferences/2024">ICML 2024</a> on low-bit integer training.
              </p>
              <p>
                <strong>[March. 2024]</strong> </b> We release the <a href="https://arxiv.org/abs/2402.17645">paper</a> and the <a href="https://github.com/pjlab-songcomposer/songcomposer">code</a> of SongComposer, a large language model for song generation.  
              </p>
<!--               <p>
                <strong>[Jul. 2023]</strong> </b> Two papers have been accepted at <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>. One focuses on video Transformer pruning, and the other on self-supervised video object discovery.
              </p> -->
              <!-- <p>
                <strong>[Jun. 2023]</strong> </b> Graduate from SJTU in just two years by exception! Immensely grateful to my advisor, Prof. Xiong, for providing invaluable support throughout my master's journey.</a>
              </p> -->
              <!-- <p>
                <strong>[Jul. 2022]</strong> </b> Our paper <em>Static and Dynamic Concepts for Self-supervised Video Representation Learning</em> was accepted at <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </p>
              <p>
                <strong>[Jun. 2022]</strong> </b> Our paper <em>Dual Contrastive Learning for Spatio-temporal Representation</em> was accepted at <a href="https://2022.acmmm.org/">ACM MM 2022</a>.
              </p>
              <p>
                <strong>[Mar. 2022]</strong> </b> Our paper <em>Motion-aware Contrastive Video Representation Learning via Foreground-background Merging
                    </em> was accepted at <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Motion-Aware_Contrastive_Video_Representation_Learning_via_Foreground-Background_Merging_CVPR_2022_paper.html">CVPR 2022</a>.
              </p> -->
              <!-- <p>
                <strong>[Jul. 2021]</strong> </b> Our paper <em>Enhancing Self-Supervised Video Representation Learning via Multi-Level Feature Optimization
                    </em> was accepted at <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Qian_Enhancing_Self-Supervised_Video_Representation_Learning_via_Multi-Level_Feature_Optimization_ICCV_2021_paper.html">ICCV 2021</a>.
              </p> -->
              <!-- <p>
                <strong>[Sep. 2020]</strong> </b> Our paper <em>Towards More
                     Practical Adversarial Attacks on Graph Neurel Networks
                    </em> was accepted at <a href="https://papers.nips.cc/paper/2020/hash/32bb90e8976aab5298d5da10fe66f21d-Abstract.html">NeurIPS 2020</a>.
              </p> -->
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:baseline">
          <heading>Preprint</heading>
        </td>
      </tr>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/llm.png' width="200">
                </div>
              </td>

              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Streaming Long Video Understanding with Large Language Models</papertitle>
                <br>
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <a href="https://lightdxy.github.io/">Xiaoyi Dong</a>,
                <a href="https://panzhang0212.github.io/">Pan Zhang</a>,
                <a href="https://yuhangzang.github.io/">Yuhang Zang</a>,
                <strong>Shuangrui Ding</strong>,
                <a href="http://dahua.site/">Dahua Lin</a>,
                <a href="https://myownskyw7.github.io/">Jiaqi Wang</a>
                <br>
                  <em>preprint</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2405.16009">arXiv</a>
                <p></p>
                <p> Long video understanding with disentangled streaming video encoding and LLM reasoning.</p>
              </td>
          </tr> 
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/songcomposer.png' width="200">
                </div>
              </td>

              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation</papertitle>
                <br>
                <strong>Shuangrui Ding</strong>*, Zihan Liu*, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, 
                <a href="http://dahua.site/">Dahua Lin</a>,
                <a href="https://myownskyw7.github.io/">Jiaqi Wang</a>
                <br>
                  <em>preprint</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2402.17645">arXiv</a> 
                /
                <a href="https://github.com/pjlab-songcomposer/songcomposer">code</a> 
                /
                <a href="https://www.bilibili.com/video/BV1bw4m1d7dK">invited talk</a>
                /
                <a href="https://pjlab-songcomposer.github.io/">demo page</a>
                <p></p>
                <p> A language large model that understands and generates melodies and lyrics in symbolic song representations.</p>
              </td>
            </tr> 
        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/internlm.png' width="200">
                </div>
              </td>

              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</papertitle>
                <br>
                Pan Zhang*, Xiaoyi Dong*, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, <strong>Shuangrui Ding</strong>, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang
                <br>
                  <em>preprint</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2309.15112">arXiv</a> 
                /
                <a href="https://github.com/InternLM/InternLM-XComposer">code</a> 
                <p></p>
                <p> A vision-language large model that enables advanced image-text comprehension and composition.</p>
              </td>
            </tr> 
        </tbody></table> 

    


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:baseline">
          <heading>Publications(&#42; equal contribution)</heading>
        </td>
      </tr>
    </tbody></table>
        
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/ECCV_ICMH.png' width="200">
              </div>
            </td>

            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Image Compression for Machine and Human Vision with Spatial-Frequency Adaptation</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=k-X6s24AAAAJ">Han Li</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=UDQR5QkAAAAJ">Shaohui Li</a>,
              <strong>Shuangrui Ding</strong>,
              <a href="https://scholar.google.com/citations?user=Xg8MhyAAAAAJ&hl=en">Wenrui Dai</a>,
              Maida Cao,
              <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=17">Chenglin Li</a>,
              <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=15">Junni Zou</a>,
              <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
              <br>
                <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.09853">arXiv</a>
              /
              <a href="https://github.com/qingshi9974/ECCV2024-AdpatICMH">code</a>
              <p></p>
              <p> Efficiently adapt image foundation models to video domain in an object-centric manner.</p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/ECCV2024_I2V.png' width="200" height="80">
              </div>
            </td>

            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Rethinking Image-to-Video Adaptation: An Object-centric Perspective</papertitle>
              <br>
              
              <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
              <strong>Shuangrui Ding</strong>,
              <a href="http://dahua.site/">Dahua Lin</a>,
              <br>
                <em>ECCV</em>, 2024
              <br>
              <a href="https://www.arxiv.org/abs/2407.06871">arXiv</a>
              <p></p>
              <p> Efficiently adapt image foundation models to video domain in an object-centric manner.</p>
            </td>
        </tr>
        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/ECCV2024_BA.png' width="200" height="80">
              </div>
            </td>

            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</papertitle>
              <br>
              <strong>Shuangrui Ding</strong>*,
              <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>*,
              Haohang Xu,
              <a href="http://dahua.site/">Dahua Lin</a>,
              <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
              <br>
                <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2311.17893">arXiv</a> 
              /
              <a href="https://github.com/shvdiwnkozbw/SSL-UVOS">code</a>
              <p></p>
              <p> Learn robust spatio-temporal corrependence on top of DINO-pretrained Transformer without any annotation.</p>
            </td>
        </tr> 
        <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/icml.png' width="200" height="80">
                </div>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>AMPA: Adaptive Mixed Precision Allocation For Low-Bit Integer Training</papertitle>
                <br>
                Li Ding, Wen Fei, Yuyang Huang, <strong>Shuangrui Ding</strong>, 
                <a href="https://scholar.google.com/citations?user=Xg8MhyAAAAAJ&hl=en">Wenrui Dai</a>,
                <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=17">Chenglin Li</a>,
                <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=15">Junni Zou</a>,
                <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
                <br>
        <em>ICML</em>, 2024
                <br>
                <a href="https://openreview.net/pdf?id=HfxFasUfbN">pdf</a>
                <p></p>
                <p> Propose a novel low-bit integer training framework that achieves adaptive mixed-precision allocation for weights, activations, and gradients, and pushes the boundaries to a precision level below INT8.</p>
              </td>
        </tr>

        <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/ICCV23_token.png' width="200" height="80">
                </div>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation</papertitle>
                <br>
                <strong>Shuangrui Ding</strong>,
                <a href="https://scholar.google.com/citations?user=hCr8Km8AAAAJ&hl=zh-CN">Peisen Zhao</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ud6aBAcAAAAJ">Xiaopeng Zhang</a>,
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>,
                <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ">Qi Tian</a>
                <br>
        <em>ICCV</em>, 2023
                <br>
                <a href="https://mark12ding.github.io/project/ICCV23_STA/">Project page</a>
                /
                <a href="https://arxiv.org/abs/2308.04549">arXiv</a> 
                /
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.pdf">pdf</a> 
                /
                <a href="https://github.com/Mark12Ding/STA">code</a>
                /
                <a href="data/ICCV_STA_poster.pdf">poster</a> 
                /
                <a href="https://docs.google.com/presentation/d/1um8GFUI6Qus5pJP7thFusnhGUzanpjLtuAVLsqcsYAQ/edit?usp=sharing">slides</a> 
                <p></p>
                <p> Propose token pruning strategy for video Transformers to offer a competitive speed-accuracy trade-off without additional training or parameters.</p>
              </td>
        </tr>
        <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/ICCV23_ssl.png' width="200" height="100">
                </div>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos</papertitle>
                <br>
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <strong>Shuangrui Ding</strong>, 
                Xian Liu,
                <a href="http://dahua.site/">Dahua Lin</a>
                <br>
        <em>ICCV</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2308.09951">arXiv</a> 
                /
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Semantics_Meets_Temporal_Correspondence_Self-supervised_Object-centric_Learning_in_Videos_ICCV_2023_paper.pdf">pdf</a> 
                /
                <a href="https://github.com/shvdiwnkozbw/SMTC">code</a>
                /
                <a href="data/ICCV_SMTC_poster.pdf">poster</a> 
                <p></p>
                <p> Jointly utilizes high-level semantics and low-level temporal correspondence for object-centric learning in videos without any supervision. </p>
              </td>
        </tr>
        <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <img src='images/ECCV22.png' width="200">
                </div>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Static and Dynamic Concepts for Self-supervised Video Representation Learning</papertitle>
                <br>
                <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>,
                <strong>Shuangrui Ding</strong>, Xian Liu,
                <a href="http://dahua.site/">Dahua Lin</a>
                <br>
        <em>ECCV</em>, 2022 
                <br>
                <a href="https://arxiv.org/abs/2207.12795">arXiv</a> 
                /
                <a href="https://github.com/shvdiwnkozbw/Self-supervised-Video-Concept">code</a>
                /
                <a href="data/eccv22.pdf">slide</a>
                <p></p>
                <p> Learn static and dynamic visual concepts in videos to aggregate local patterns with similar semantics to boost unsupervised video representation.</p>
              </td>
        </tr> 
        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/MM2022.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Dual Contrastive Learning for Spatio-temporal Representation</papertitle>
              <br>
              <strong>Shuangrui Ding</strong>,
              <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
              <br>
        <em>ACM MM</em>, 2022 
              <br>
              <a href="https://arxiv.org/abs/2207.05340">arXiv</a> 
              /
              <a href="data/acmmm2022_poster_mmfp0200.pdf">poster</a>
              /
              <a href="data/MM22-mmfp0200.mp4">video</a>
              /
              <a href="https://github.com/Mark12Ding/DCLR">code</a>
              
              <p></p>
              <p> Present a novel dual contrastive formulation to decouple the static/dynamic features and thus mitigate the background bias.</p>
            </td>
      </tr> 
      <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/CVPR2022.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Motion-aware Contrastive Video Representation Learning via Foreground-background Merging</papertitle>
              <br>
              <strong>Shuangrui Ding</strong>,
              Maomao Li, Tianyu Yang, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, Haohang Xu, Qingyi Chen, Jue Wang, <a href="https://min.sjtu.edu.cn/En/FacultyShow/4?Vid=14">Hongkai Xiong</a>
              <br>
        <em>CVPR</em>, 2022 
              <br>
              <a href="https://mark12ding.github.io/project/CVPR22_FAME/">Project page</a>
              / 
              <a href="https://arxiv.org/abs/2109.15130/">arXiv</a> 
              / 
              <a href="https://github.com/Mark12Ding/FAME">code</a>
              /
              <a href="https://mp.weixin.qq.com/s/eZ_8qyVa7L8-n2RHBge0mQ">Chinese coverage</a>
              /
              <a href="data/CVPR_4560_poster.pdf">poster</a>
              <p></p>
              <p>Mitigate the background bias in self-supervised video representation learning via copy-pasting the foreground onto the other backgrounds.</p>
            </td>
      </tr>
      <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='iccv_image'>
                  <img src='images/iccv2.jpeg' width="200"></div>
                <img src='images/iccv.jpeg' width="200">
              </div>
              <script type="text/javascript">
                function iccv_start() {
                  document.getElementById('iccv_image').style.opacity = "1";
                }

                function iccv_stop() {
                  document.getElementById('iccv_image').style.opacity = "0";
                }
                iccv_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization</papertitle>
              <br>
              <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, Yuxi Li, Huabin Liu, John See,
              <strong>Shuangrui Ding</strong>, Xian Liu, Dian Li, 
              <a href="https://weiyaolin.github.io/">Weiyao Lin</a>
              <br>
        <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.02183">arXiv</a> / 
              <a href="https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization">code</a>
              <p></p>
              <p>Self-supervised video representation learning from the perspective of both high-level semantics and lower-level characteristics</p>
            </td>
      </tr>
      <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/NIPS2020.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Towards More Practical Adversarial Attacks on Graph Neural Networks</papertitle>
              <br>
              <a href="http://www.jiaqima.com/">Jiaqi Ma*</a>,
              <strong>Shuangrui Ding*</strong>,
              <a href="http://www-personal.umich.edu/~qmei">Qiaozhu Mei</a>
              <br>
        <em>NeurIPS</em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/2006.05057">arXiv</a>
        /
              <a href="https://docs.google.com/presentation/d/18AqRjgTz8d2sSCE_cUgTHYzs2mWhOug-U5HZWBUU67o/edit?usp=sharing">slides</a>
        /    
              <a href="https://slideslive.com/38931435/practical-adversarial-attacks-on-graph-neural-networks">video
              </a>
        /
              <a href="https://github.com/Mark12Ding/GNN-Practical-Attack">code
              </a>
              <p></p>
              <p>Exploiting the structural inductive biases of GNNs, the restricted black-box adversarial attacks can be conducted effectively.</p>
            </td>
      </tr> 
      </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards</heading>
              <p>
                <strong>CUHK Vice-Chancellor's Ph.D. Scholarship</strong> (80,000 HKD), Graduate school of CUHK. 2023 
                <!-- [<a href="https://mp.weixin.qq.com/s/8hZz15kjuEoPP_aCZ2sSRQ">coverage</a>] -->
              </p>    
              
              <p>
                <strong>Graduate National Scholarship</strong> (Top 2%), Ministry of Education of China. 2022 
                <!-- [<a href="https://mp.weixin.qq.com/s/8hZz15kjuEoPP_aCZ2sSRQ">coverage</a>] -->
              </p>              
              <p>
                <strong>Shanghai Excellent Graduate</strong> (Top 5%), Shanghai Municipal Education Commission. 2021 
              </p>
              <p>
                <strong>Finalist winner</strong> (Top 0.3%), Mathematical Contest in Modeling. 2019 
              </p>
              <p>
              <strong>National Scholarship</strong> (Top 2%), Ministry of Education of China. 2018 
              </p>
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Services</heading>
              <p>
                  <li>
                      Reviewer: ECCV’22/24, CVPR’23-24, ICCV’23, NeurIPS’23-24, ICLR’23-24, AAAI’23-24, ICML’24, ACM MM’23-24.
                  </li>
              </p>
            </td>
          </tr>
        </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Misc</heading>
              <p>
                1. My favorite sports is soccer. I was the captain of UM-SJTU JI soccer team during season 2018. Besides, I am a super fan of Manchester City in Premier League.
              </p>
              <p>
                2. I am proud that I have graudated from the competition class at Hangzhou No.2 High school, where I make friends with so many talented students and prestigious teachers. 
              </p>
              <p>
                3. It is worth mentioning that <a href="https://shvdiwnkozbw.github.io/">Rui</a> is my best friend and has motivated me forward for over ten years as my role model. Best wishes and good luck!
              </p>
            </td>
          </tr>
        </tbody></table>
<!--     <table width="50%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td style="padding:0px">
                    <br>
                    <br>
                    <div>
                        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=7lqf9KytMX7LblPx6VWALqJ3PbQAiEEYCwUq_KvZgBI'></script>
                    </div>
                </td>
            </tr>
        </tbody>
    </table> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td style="padding:0px">
                    <p font-size:small;>
                        <br>
                        <br>
                        <div style="float:left;">
                            Updated at Jul. 2024
                        </div>
                        <div style="float:right;">
                            Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                        </div>
                        <br>
                        <br>        
                    </p>                           
                </td>
            </tr>
        </tbody>
    </table>
    </body>
</html>

